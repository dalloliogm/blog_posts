{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bc3a29f-1cc0-44ae-99b8-e654a0465d1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Downstream prediction \n",
    "\n",
    "Run this notebook on google colab to use a free GPU! \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/helicalAI/helical/blob/main/examples/notebooks/Hyena-DNA-Inference.ipynb)\n",
    "\n",
    "In this notebook, a HyenaDNA model is used for various classifications tasks with a given sequence of nucleotides.\n",
    "\n",
    "A HyenaDNA model (2 layers and width 256) is used to create embeddings of nucleotides.\n",
    "\n",
    "A neural network is then trained, using the embeddings as inputs, to make a prediction.\n",
    "\n",
    "This notebook can be used for any of the 18 [nucleotide transformer downstream tasks](https://huggingface.co/datasets/InstaDeepAI/nucleotide_transformer_downstream_tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c767ce-7de6-40e8-9bb4-809d392ffc94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install helical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fac5331-f0a9-484f-8867-916759df8f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from datasets import DatasetDict\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Tuple\n",
    "import logging, warnings\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8165b70e-8b91-48cf-b8ad-ebe6b8bac40b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Use the Helical package to get the Hyena model\n",
    "We use a small HyenaDNA model with 2 layers and width 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "164c09f9-895e-4c86-9c0f-53956bcb7c38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from helical.models.hyena_dna.model import HyenaDNA\n",
    "from helical.models.hyena_dna.hyena_dna_config import HyenaDNAConfig  \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "configurer = HyenaDNAConfig(model_name=\"hyenadna-tiny-1k-seqlen-d256\", device=device, batch_size=5)\n",
    "hyena_model = HyenaDNA(configurer=configurer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d80343c-2e5d-4cd4-b596-df264398e82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Download the dataset\n",
    "Several datasets are available from the [Nucleotide Transformer](https://arxiv.org/abs/2306.15794). Using the `get_dataset_config_names()` function, we get a list of the available the datasets for the downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92fe281d-6a59-4da1-aa9e-b245522bb6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "configs = get_dataset_config_names(\"InstaDeepAI/nucleotide_transformer_downstream_tasks\", trust_remote_code=True)\n",
    "configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e594e44-8a41-48ce-aa61-eaa27b6a14fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can select any of the 18 downstream tasks. Let us take the `promoter_tata` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d327abb-e4cf-4b86-9317-5d6e1571fd33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "label = \"promoter_tata\"\n",
    "dataset = load_dataset(\"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\", label, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2749f49c-2984-4fb1-9e22-943f2c86573b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To familiarize ourselves with the data, we can print the first seqence and see if it is a splice site acceptor or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe703de-dfa4-4f02-b01c-f59223caff00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Nucleotide sequence:\", dataset[\"train\"][\"sequence\"][0][:10], \"...\")\n",
    "print(\"Label name:\", dataset[\"train\"].config_name, \"and value:\", dataset[\"train\"][\"label\"][0])\n",
    "num_classes = len(set(dataset[\"train\"][\"label\"]))\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "824891f5-26c1-4c0e-8a4c-bf115938e81d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define a function that gets the embeddings for each nucleotide sequence in the training dataset.\n",
    "\n",
    "According to the HyenaDNA [paper](https://arxiv.org/pdf/2306.15794): \"[they] average across the tokens to obtain a single classification token\".\n",
    "\n",
    "In our code below, the Hyena model returns a (302, 256) matrix. We average column wise resulting in a vector of shape (256, ) for each observation.\n",
    "\n",
    "During the training process, we also found that it is beneficial to normalize the data row-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "660f8bd9-a21a-41c8-b929-1479362e2ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_model_inputs(dataset: DatasetDict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : DatasetDict\n",
    "        The dataset containing the sequences and labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray]\n",
    "        A tuple containing the input features and labels.\n",
    "    \"\"\"\n",
    "    processed_data = hyena_model.process_data(dataset[\"sequence\"])\n",
    "\n",
    "    embeddings = hyena_model.get_embeddings(processed_data)\n",
    "    embeddings = embeddings.mean(axis=1)\n",
    "    means = np.mean(embeddings, axis=1, keepdims=True)\n",
    "    stds = np.std(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    x = (embeddings - means) / stds\n",
    "\n",
    "    return x, dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9532838c-4459-4872-a682-35032700ca40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It may be beneficial to do this step once and save the output in a `.npy` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d86e04e-769c-44b6-8842-33799ef966f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x, y = get_model_inputs(dataset[\"train\"])\n",
    "#np.save(f\"data/train/x_{label}_norm_256\", x)\n",
    "#np.save(f\"data/train/y_{label}_norm_256\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "176049eb-6b7d-496b-96f3-ac0d3fa3ab66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Load the data and one-hot-encode the labels.\n",
    "\n",
    "We split the training set into actual training data and a test set. \n",
    "\n",
    "This is optional and the entire dataset can be used for training. We did this to avoid data leakage by not touching the test set during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "362c9b5a-7240-4108-b2bf-cbb759b7df76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#x = np.load(f\"data/train/x_{label}_norm_256.npy\")\n",
    "#y = np.load(f\"data/train/y_{label}_norm_256.npy\")\n",
    "\n",
    "# # One-hot encode the labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "y_encoded = one_hot(torch.tensor(y_encoded),num_classes).float()\n",
    "y_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16dac3eb-98bb-4c0e-beb0-f568b075c555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba6b1d76-7809-48c3-ac03-63bbb3a6368b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_shape = configurer.config['d_model']\n",
    "\n",
    "# Define the model architecture\n",
    "head_model = nn.Sequential(\n",
    "    nn.Linear(input_shape, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(64, num_classes),\n",
    "    nn.Softmax(dim=1)\n",
    "    )\n",
    "\n",
    "print(head_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d44785da-0071-4623-be13-581836a37c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model: nn.Sequential,\n",
    "                X_train: torch.Tensor,  \n",
    "                y_train: torch.Tensor,  \n",
    "                X_val: torch.Tensor, \n",
    "                y_val: torch.Tensor, \n",
    "                optimizer = optim.Adam, \n",
    "                loss_fn = nn.CrossEntropyLoss(),\n",
    "                num_epochs = 25, \n",
    "                batch = 64):    \n",
    "\n",
    "    # Create DataLoader for batching\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "\n",
    "    # Validation dataset\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
    "\n",
    "    # Ensure model is in training mode\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Validation phase (optional)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for val_X, val_y in val_loader:\n",
    "                val_outputs = model(val_X)\n",
    "                val_loss = loss_fn(val_outputs, val_y)\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}, Validation Loss: {sum(val_losses)/len(val_losses)}\")\n",
    "        \n",
    "        # Set back to training mode for next epoch\n",
    "        model.train()\n",
    "        \n",
    "    model.eval()   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b27e6eec-560e-478e-a24c-a2048061e6d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b1222c9-891b-4876-810c-9ea0ce0a7e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trained_model = train_model(head_model, \n",
    "                            torch.from_numpy(X_train), \n",
    "                            y_train, \n",
    "                            torch.from_numpy(X_test), \n",
    "                            y_test,\n",
    "                            optim.Adam(head_model.parameters(), lr=0.001),\n",
    "                            nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c15638aa-c246-46ab-b415-7f29dbda3ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_unseen, y_unseen = get_model_inputs(dataset[\"test\"])\n",
    "#np.save(f\"data/test/x_{label}_norm_256.npy\")\n",
    "#np.save(f\"data/test/y_{label}_norm_256.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd54ee7-797b-48ca-9bcf-b85c56877090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate the model on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a917e593-7a0c-4022-b8e4-0f7a1ecde9d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#X_unseen = np.load(f\"data/test/x_{label}_norm_256.npy\")\n",
    "#y_unseen = np.load(f\"data/test/y_{label}_norm_256.npy\")\n",
    "\n",
    "predictions_nn = trained_model(torch.from_numpy(X_unseen))\n",
    "\n",
    "y_pred = np.array(torch.argmax(predictions_nn, dim=1))\n",
    "print(\"Correct predictions: {:.2f}%\".format(sum(np.equal(y_pred, y_unseen))*100/len(y_unseen)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c03e4c-9476-4dac-a854-15361e586e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The [Hyena](https://arxiv.org/pdf/2306.15794) and the [Nucleotide transformer](https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1.full.pdf) papers, report accuracies around 95% for this task. Our results underperform in comparison. This is probably due to the much larger models being used for the NT, while the Hyena model was re-trained from scratch for this task. In future work, we want to achieve these accuracies too with either approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e1842ca-9bea-44cf-88e9-d53d8118306d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## OPTIONAL\n",
    "For reference, we also trained an SVM model and obtained similar results (to our small NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8ec92a5-024a-45ab-8ddc-889b4e37d463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train the SVM model\n",
    "svm_model = svm.SVC(kernel='rbf', degree=3, C=1, decision_function_shape='ovr')  # One-vs-rest strategy\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f03c608-2482-405e-a475-d07e0352beeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "unseen_predictions_svm = svm_model.predict(X_unseen)\n",
    "\n",
    "accuracy = accuracy_score(y_unseen, unseen_predictions_svm)\n",
    "print(\"Test accuracy: {:.1f}%\".format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "635987c6-8374-4e93-b12a-875c4cfdf8bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# NN\n",
    "# y_true = np.argmax(y_test, axis=1)\n",
    "# y_pred_classes = np.argmax(predictions_nn, axis=1)\n",
    "\n",
    "# SVM\n",
    "y_true = y_test\n",
    "y_pred_classes = svm_model.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "/Workspace/Users/giovanni@gmdbioinformatics.com/environment_helixai_tutorial_geneformer.yaml",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Hyena-DNA-Inference",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "helical-package",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
